#!/bin/bash

# This is p-norm neural net training, with the "fast" script, on top of adapted
# 40-dimensional features.


train_stage=20
use_gpu=false

. cmd.sh
. ./path.sh
. utils/parse_options.sh


if $use_gpu; then
  if ! cuda-compiled; then
    cat <<EOF && exit 1 
This script is intended to be used with GPUs but you have not compiled Kaldi with CUDA 
If you want to use GPUs (and have them), go to src/, and configure and make on a machine
where "nvcc" is installed.
EOF
  fi
  parallel_opts="--gpu 1"
  num_threads=1
  minibatch_size=512
  dir=exp/nnet2_clean_urdu_gpu
else
  # with just 4 jobs this might be a little slow.
  num_threads=4
  parallel_opts="--num-threads $num_threads" 
  minibatch_size=128
  # urdu clean model
  dir=exp/nnet2_clean_urdu
fi

. ./cmd.sh
. utils/parse_options.sh

 if [ ! -f $dir/final.mdl ]; then
   steps/nnet2/train_pnorm_fast.sh --stage $train_stage \
    --samples-per-iter 200000 \
    --parallel-opts "$parallel_opts" \
    --num-threads "$num_threads" \
    --minibatch-size "$minibatch_size" \
    --num-jobs-nnet 4  --mix-up 4000 \
    --initial-learning-rate 0.01 --final-learning-rate 0.001 \
    --num-hidden-layers 3 \
    --pnorm-input-dim 1200 --pnorm-output-dim 400 \
    --cmd "$decode_cmd" \
     data/train data/lang exp/tri3b_ali $dir || exit 1
 fi


# for t in test train train.1k; do
  steps/nnet2/decode.sh --nj 4 --cmd "$decode_cmd" \
    --transform-dir exp/tri3b/decode \
    exp/tri3b/graph data/test $dir/decode_train_new || exit 1;
  # steps/lmrescore.sh --cmd "$decode_cmd" data/lang_test_{tgsmall,tgmed} \
  #   data/$test $dir/decode_{tgsmall,tgmed}_$test  || exit 1;
  # steps/lmrescore_const_arpa.sh \
  #   --cmd "$decode_cmd" data/lang_test_{tgsmall,tglarge} \
  #   data/$test $dir/decode_{tgsmall,tglarge}_$test || exit 1;
  # steps/lmrescore_const_arpa.sh \
  #   --cmd "$decode_cmd" data/lang_test_{tgsmall,fglarge} \
  #   data/$test $dir/decode_{tgsmall,fglarge}_$test || exit 1;
# done

exit 0;

